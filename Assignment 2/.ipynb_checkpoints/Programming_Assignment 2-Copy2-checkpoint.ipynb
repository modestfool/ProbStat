{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import os\n",
    "import re\n",
    "import traceback\n",
    "from happierfuntokenizing import Tokenizer\n",
    "from __future__ import division\n",
    "\n",
    "total_number_of_blog_posts = 0\n",
    "total_number_of_users = 0\n",
    "industry_users_map = {}\n",
    "userid_industry_map = {}\n",
    "userid_age_map = {}\n",
    "userid_gender_map = {}\n",
    "\n",
    "all_words_counts = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Method for reading all files, parsing the blogs and tokenizing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_all_blogs(users_words_counts,users_topics_probs):\n",
    "    \n",
    "    global total_number_of_blog_posts\n",
    "    global total_number_of_users\n",
    "    global industry_users_map \n",
    "    global userid_industry_map\n",
    "    global userid_age_map\n",
    "    global userid_gender_map\n",
    "        \n",
    "    #global users_topics_probs\n",
    "    \n",
    "    global all_words_counts\n",
    "\n",
    "    path = \"blogs/\"\n",
    "    tokenizer_instance = Tokenizer()\n",
    "    \n",
    "    pattern = r'<post>(.*?)</post>'\n",
    "    \n",
    "    iterations = 0\n",
    "    try:\n",
    "        \n",
    "        topic_words_file = pd.read_csv(\"wwbpFBtopics_condProb.csv\")\n",
    "        \n",
    "        for filename in os.listdir(path):  #### For EACH USER\n",
    "            \n",
    "            iterations = iterations+1\n",
    "            print iterations\n",
    "            \n",
    "            if iterations>500:\n",
    "                break\n",
    "                \n",
    "            word_count_map = {}\n",
    "            topic_prob_map = {}\n",
    "            \n",
    "            if filename.startswith(\".\"):\n",
    "                continue\n",
    "                \n",
    "            all_posts = []\n",
    "            parts = filename.split(\".\")\n",
    "            \n",
    "            user_id = (int)(parts[0])\n",
    "            gender = parts[1]\n",
    "            age = (int)(parts[2])\n",
    "            industry = parts[3]\n",
    "            \n",
    "            user_total_words_count = 0\n",
    "            userid_age_map[user_id] = age\n",
    "            \n",
    "            if gender == \"male\":\n",
    "                userid_gender_map[user_id] = 0\n",
    "            else:\n",
    "                userid_gender_map[user_id] = 1\n",
    "                \n",
    "            userid_industry_map[user_id] = industry\n",
    "\n",
    "            \n",
    "            if industry in industry_users_map:\n",
    "                industry_users_map[industry] = industry_users_map[industry] + 1\n",
    "            else:\n",
    "                industry_users_map[industry] = 1\n",
    "                \n",
    "            \n",
    "            full_filename = path+filename\n",
    "            \n",
    "            with open(full_filename, 'r') as myfile:\n",
    "                text = myfile.read().replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "    \n",
    "            all_posts = re.findall(pattern, text, re.DOTALL)\n",
    "        \n",
    "            total_number_of_blog_posts = total_number_of_blog_posts + len(all_posts)\n",
    "            \n",
    "            ### All the posts in this user's blog\n",
    "            for blog_post in all_posts:\n",
    "                words = tokenizer_instance.tokenize(blog_post)\n",
    "\n",
    "                ### Total Number of Words This User uses\n",
    "                user_total_words_count = user_total_words_count + len(words)\n",
    "                \n",
    "                for word in words:\n",
    "                    if word in word_count_map:\n",
    "                        word_count_map[word] = word_count_map[word] + 1\n",
    "                    else:\n",
    "                        word_count_map[word] = 1   \n",
    "            \n",
    "                    if word in all_words_counts:\n",
    "                        all_words_counts[word] = all_words_counts[word] + 1\n",
    "                    else:\n",
    "                        all_words_counts[word] = 1      \n",
    "                    \n",
    "            #for word, user_wc in word_count_map.iteritems():\n",
    "            for topic in range(2000):\n",
    "                \n",
    "                ### Initializing Prob of topic given user = 0\n",
    "                prob_topic_given_user = 0.0\n",
    "\n",
    "                word_row = topic_words_file[topic_words_file[\"category\"]==topic]\n",
    "                \n",
    "                for index, row in word_row.iterrows():\n",
    "                    word = row[0]\n",
    "                    prob_topic_given_word = row[2]\n",
    "                    \n",
    "                    if word in word_count_map:\n",
    "                        user_wc = word_count_map[word]\n",
    "                        \n",
    "                        prob_word_given_user = user_wc / user_total_words_count\n",
    "\n",
    "                        cur = prob_topic_given_word * prob_word_given_user\n",
    "                        prob_topic_given_user = prob_topic_given_user + cur\n",
    "                    \n",
    "                topic_prob_map[topic] = prob_topic_given_user\n",
    "                        \n",
    "                \n",
    "            users_words_counts[user_id] = word_count_map\n",
    "            users_topics_probs[user_id] = topic_prob_map\n",
    "                \n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print \"1.a) posts: %d \" % total_number_of_blog_posts\n",
    "    print \"1.b) users: %d \" % len(userid_age_map)\n",
    "    print \"1.c) words: %d \" % len(all_words_counts)\n",
    "    print \"1.d) \"\n",
    "    print industry_users_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print userid_age_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    users_words_counts = {}\n",
    "    users_topics_probs = {}\n",
    "\n",
    "    read_all_blogs(users_words_counts,users_topics_probs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2) Calculating first 3 users' probability of mentioning 3 given topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topicwordsfile = pd.read_csv(\"wwbpFBtopics_condProb.csv\")\n",
    "\n",
    "topic_map = {}\n",
    "\n",
    "topic_map[463] = topicwordsfile[topicwordsfile[\"category\"]==463]\n",
    "topic_map[963] = topicwordsfile[topicwordsfile[\"category\"]==963]\n",
    "topic_map[981] = topicwordsfile[topicwordsfile[\"category\"]==981]\n",
    "\n",
    "#print topic_map[963]\n",
    "\n",
    "lowest_user_ids = [5114,7596,8173]\n",
    "\n",
    "print \"2.a)\"\n",
    "for userid in lowest_user_ids:\n",
    "    print \"%d\" %userid\n",
    "    \n",
    "    for topic in topic_map:\n",
    "        print str(topic) + \" : \"\n",
    "        \n",
    "        prob_topic_given_user = users_topics_probs[topic]\n",
    "        print prob_topic_given_user\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3) Correlate each topic usage with user age, adjusting for gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def multiply_matrices(Topics, Genders, Ages):\n",
    "    Ones = []\n",
    "    for i in range(len(Topics)):\n",
    "        Ones.append(1)\n",
    "        \n",
    "    Right = np.matrix([Topics,Genders,Ones])\n",
    "    Right = Right.T\n",
    "    \n",
    "    Left = np.matrix([Ages])\n",
    "    Left = Left.T\n",
    "    \n",
    "    print Right\n",
    "    \n",
    "    Right_T = Right.T\n",
    "    \n",
    "    XT_X = Right_T * Right\n",
    "    XT_X_I = XT_X.I\n",
    "    \n",
    "    XT_X_I_XT = XT_X_I * Right_T\n",
    "    \n",
    "    Betas = XT_X_I_XT * Left\n",
    "    \n",
    "    return Betas\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306244\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "[[  2.99160083e-05   1.00000000e+00   1.00000000e+00]\n",
      " [  2.51238251e-05   0.00000000e+00   1.00000000e+00]\n",
      " [  5.48985766e-05   0.00000000e+00   1.00000000e+00]\n",
      " ..., \n",
      " [  1.14318328e-05   0.00000000e+00   1.00000000e+00]\n",
      " [  1.54081162e-05   0.00000000e+00   1.00000000e+00]\n",
      " [  2.91616897e-05   1.00000000e+00   1.00000000e+00]]\n",
      "[-846.926092720302]\n"
     ]
    }
   ],
   "source": [
    "# T is topic usage , A is User's Age , G is User's Gender\n",
    "topic_words_file = pd.read_csv(\"wwbpFBtopics_condProb.csv\")\n",
    "print len(topic_words_file)\n",
    "\n",
    "Ages = userid_age_map.values()\n",
    "Genders = userid_gender_map.values()\n",
    "\n",
    "Beta_topics = []\n",
    "Beta_C = []\n",
    "\n",
    "for topic in range(2000):\n",
    "    print topic\n",
    "\n",
    "    Topics = []\n",
    "    topic_pd = topic_words_file[topic_words_file[\"category\"]==topic]\n",
    "    \n",
    "    iterc =0\n",
    "    for user_id,age in userid_age_map.iteritems():\n",
    "\n",
    "        iterc = iterc + 1\n",
    "        if iterc%1000 == 0:\n",
    "            print iterc\n",
    "            \n",
    "        gender = userid_gender_map[user_id]\n",
    "        user_word_counts_map = users_words_counts[user_id]\n",
    "\n",
    "        user_all_words_count = 0\n",
    "        for w ,c in user_word_counts_map.iteritems():\n",
    "            user_all_words_count = user_all_words_count + c\n",
    "    \n",
    "        prob_topic_given_user = 0\n",
    "        \n",
    "        for index, row in topic_pd.iterrows():\n",
    "            word = row[0]\n",
    "            prob_topic_given_word = row[2]\n",
    "\n",
    "            if word in user_word_counts_map:\n",
    "                user_wc = user_word_counts_map[word]\n",
    "\n",
    "                prob_word_given_user = user_wc / user_all_words_count\n",
    "\n",
    "                cur = prob_topic_given_word * prob_word_given_user\n",
    "                prob_topic_given_user = prob_topic_given_user + cur\n",
    "\n",
    "        \n",
    "        Topics.append(prob_topic_given_user)\n",
    "    \n",
    "    Betas = multiply_matrices(Topics,Genders,Ages)\n",
    "    \n",
    "    Beta_topic = Betas.item(0)\n",
    "    Beta_c = Betas.item(2)\n",
    "    \n",
    "    Beta_C.append(Beta_c)\n",
    "    Beta_topics.append(Beta_topic)\n",
    "    \n",
    "    print Beta_topics\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
