{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import os\n",
    "import re\n",
    "import traceback\n",
    "from happierfuntokenizing import Tokenizer\n",
    "from __future__ import division\n",
    "\n",
    "total_number_of_blog_posts = 0\n",
    "total_number_of_users = 0\n",
    "industry_users_map = {}\n",
    "userid_industry_map = {}\n",
    "userid_age_map = {}\n",
    "userid_gender_map = {}\n",
    "\n",
    "all_words_counts = {}\n",
    "total_number_of_words_all_users = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_all_blogs(users_word_counts):\n",
    "    \n",
    "    global total_number_of_blog_posts\n",
    "    global total_number_of_users\n",
    "    global industry_users_map \n",
    "    global userid_industry_map\n",
    "    global userid_age_map\n",
    "    global userid_gender_map\n",
    "        \n",
    "    global all_words_counts\n",
    "    global total_number_of_words_all_users\n",
    "\n",
    "    path = \"blogs/\"\n",
    "    tokenizer_instance = Tokenizer()\n",
    "    \n",
    "    pattern = r'<post>(.*?)</post>'\n",
    "\n",
    "    try:\n",
    "        for filename in os.listdir(path):\n",
    "            \n",
    "            word_counts_map = {}\n",
    "            \n",
    "            #print filename\n",
    "            if filename.startswith(\".\"):\n",
    "                continue\n",
    "                \n",
    "            all_posts = []\n",
    "            parts = filename.split(\".\")\n",
    "            #print parts\n",
    "            \n",
    "            user_id = (int)(parts[0])\n",
    "            gender = parts[1]\n",
    "            age = (int)(parts[2])\n",
    "            industry = parts[3]\n",
    "            \n",
    "            userid_age_map[user_id] = age\n",
    "            userid_gender_map[user_id] = gender\n",
    "            userid_industry_map[user_id] = industry\n",
    "            \n",
    "            if industry in industry_users_map:\n",
    "                tmp_count = industry_users_map[industry]\n",
    "                tmp_count = tmp_count + 1\n",
    "                industry_users_map[industry] = tmp_count\n",
    "            else:\n",
    "                industry_users_map[industry] = 1\n",
    "                \n",
    "            \n",
    "            full_filename = path+filename\n",
    "            #print filename\n",
    "            \n",
    "            with open(full_filename, 'r') as myfile:\n",
    "                text = myfile.read().replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "    \n",
    "            all_posts = re.findall(pattern, text, re.DOTALL)\n",
    "        \n",
    "            total_number_of_blog_posts = total_number_of_blog_posts + len(all_posts)\n",
    "            \n",
    "            for blog_post in all_posts:\n",
    "                words = tokenizer_instance.tokenize(blog_post)\n",
    "                #print blog_post\n",
    "                \n",
    "                if user_id in users_word_counts:\n",
    "                    word_counts_map = users_word_counts[user_id]\n",
    "                else:\n",
    "                    users_word_counts[user_id] = word_counts_map\n",
    "               \n",
    "                for word in words:\n",
    "                    if word in word_counts_map:\n",
    "                        count = word_counts_map[word]\n",
    "                        count = count + 1\n",
    "                        word_counts_map[word] = count\n",
    "                    else:\n",
    "                        word_counts_map[word] = 1\n",
    "                \n",
    "                for word in words:\n",
    "                    if word in all_words_counts:\n",
    "                        count = all_words_counts[word]\n",
    "                        count = count + 1\n",
    "                        all_words_counts[word] = count\n",
    "                    else:\n",
    "                        all_words_counts[word] = 1\n",
    "                        \n",
    "                #print word_counts_map\n",
    "            #print users_word_counts\n",
    "            #break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print e\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print \"1.a) posts: %d \" % total_number_of_blog_posts\n",
    "    print \"1.b) users: %d \" % len(userid_age_map)\n",
    "    print \"1.c) words: %d \" % len(all_words_counts)\n",
    "    print \"1.d) \"\n",
    "    print industry_users_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print len(userid_age_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.a) posts: 681288 \n",
      "1.b) users: 19320 \n",
      "1.c) words: 960894 \n",
      "1.d) \n",
      "{'Arts': 721, 'HumanResources': 94, 'Publishing': 150, 'indUnk': 6827, 'LawEnforcement-Security': 57, 'Museums-Libraries': 55, 'Religion': 139, 'Internet': 397, 'Environment': 28, 'Transportation': 91, 'Sports-Recreation': 90, 'Government': 236, 'Marketing': 180, 'Maritime': 17, 'Chemicals': 62, 'Construction': 55, 'Student': 5120, 'Non-Profit': 372, 'Law': 197, 'Tourism': 94, 'BusinessServices': 163, 'Communications-Media': 479, 'Science': 184, 'InvestmentBanking': 33, 'Banking': 112, 'Fashion': 98, 'Architecture': 69, 'Military': 116, 'Biotech': 57, 'Consulting': 191, 'Manufacturing': 87, 'Agriculture': 36, 'Engineering': 312, 'Automotive': 54, 'Accounting': 105, 'Education': 980, 'Technology': 943, 'Telecommunications': 119, 'RealEstate': 55, 'Advertising': 145}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    users_word_counts = {}\n",
    "\n",
    "    read_all_blogs(users_word_counts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19320\n"
     ]
    }
   ],
   "source": [
    "print len(users_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d68808b258b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtopicwordsfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wwbpFBtopics_condProb.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtopic_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtopic_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m463\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopicwordsfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopicwordsfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"category\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m463\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "topicwordsfile = pd.read_csv(\"wwbpFBtopics_condProb.csv\")\n",
    "\n",
    "topic_map = {}\n",
    "\n",
    "topic_map[463] = topicwordsfile[topicwordsfile[\"category\"]==463]\n",
    "topic_map[963] = topicwordsfile[topicwordsfile[\"category\"]==963]\n",
    "topic_map[981] = topicwordsfile[topicwordsfile[\"category\"]==981]\n",
    "\n",
    "#print topic_map[963]\n",
    "\n",
    "lowest_user_ids = [5114,7596,8173]\n",
    "\n",
    "print \"2.a)\"\n",
    "for userid in lowest_user_ids:\n",
    "    print \"%d\" %userid\n",
    "    \n",
    "    word_counts_map = users_word_counts[userid]\n",
    "\n",
    "    denom = 0\n",
    "    \n",
    "    for w ,c in all_words_counts.iteritems():\n",
    "        if w in word_counts_map:\n",
    "            denom = denom + word_counts_map[w]\n",
    "        \n",
    "    #print word_counts_map\n",
    "    \n",
    "    for topic in topic_map:\n",
    "        print str(topic) + \" : \"\n",
    "        \n",
    "        prob = 0\n",
    "        topic_pd = topic_map[topic]\n",
    "        \n",
    "        #print topic_pd\n",
    "        for index, row in topic_pd.iterrows():\n",
    "            #print row\n",
    "            word = row[0]\n",
    "            #print word\n",
    "            prob_topic_given_word = row[2]\n",
    "\n",
    "            #print prob_topic_given_word\n",
    "\n",
    "            if word in word_counts_map:\n",
    "                count_user_word = word_counts_map[word]\n",
    "            else:\n",
    "                count_user_word = 0\n",
    "            \n",
    "            all_words_wc = denom\n",
    "\n",
    "            prob_word_given_user = count_user_word / all_words_wc\n",
    "            \n",
    "            #print prob_topic_given_word , prob_word_given_user\n",
    "            cur = prob_topic_given_word * prob_word_given_user\n",
    "            \n",
    "            prob = prob + cur\n",
    "        \n",
    "        print prob\n",
    "            \n",
    "    #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
